{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu activation function\n",
    "inputs = [ 0 , 2 , - 1 , 3.3 , - 2.7 , 1.1 , 2.2 , - 100 ]\n",
    "output = np.maximum(0,inputs)\n",
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data\n",
    "import nnfs\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "X, y = spiral_data(samples=100, classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        #initialize weights and biases\n",
    "        \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        print(\"sdjfksd\", np.random.randn(n_inputs, n_neurons))\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    #Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU :\n",
    "    \n",
    "    # Forward pass\n",
    "    \n",
    "    def forward ( self , inputs ):\n",
    "        \n",
    "        # Calculate output values from input\n",
    "    \n",
    "        self.output = np.maximum( 0 , inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data( samples = 100 , classes = 3 )\n",
    "print(X)\n",
    "print(y)\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense( 2 , 3 )\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Forward pass through activation func.\n",
    "# Takes in output from previous layer\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print (activation1.output[: 5 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values from the previous output when we described what a neural network was\n",
    "\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "e = 2.71828182846\n",
    "\n",
    "# For each value within the vector, calculate the exponential value\n",
    "exp_values = []\n",
    "for output in layer_outputs:\n",
    "   exp_values.append(e**output)\n",
    "print (exp_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We now normalize the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now normalize the values\n",
    "norm_base = sum(exp_values) # sum each value\n",
    "\n",
    "norm_values = []\n",
    "for value in exp_values:\n",
    "    norm_values.append(value/norm_base)\n",
    "print(norm_values)   \n",
    "\n",
    "print('sum of normalized values', sum(norm_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# values from the earlier previous when we described what a NN was\n",
    "\n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "# For every value in a vector > calculate its exponated value\n",
    "\n",
    "exp_values = np.exp(layer_outputs)\n",
    "\n",
    "print(exp_values)\n",
    "\n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "\n",
    "print(norm_values)\n",
    "print('sum of normalized values', np.sum(norm_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = np.array([[4.8, 1.21, 2.385],\n",
    "                         [8.9, -1.81, 0.2],\n",
    "                          [1.41, 1.051, 0.026]])\n",
    "\n",
    "print('sum axis 1, but keep the dimensions as input: ')\n",
    "print(np.sum(layer_outputs, axis = 1, keepdims = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "    \n",
    "        # Get unnormalized probabilties\n",
    "        exp_values = np.exp(inputs-np.max(inputs, axis = 1, keepdims=True))\n",
    "  \n",
    "\n",
    "        # Normalize them for each sample\n",
    "        \n",
    "        probabilties = exp_values/np.sum(exp_values, axis = 1, keepdims = True)\n",
    "\n",
    "        self.output = probabilties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = Activation_Softmax()\n",
    "\n",
    "softmax.forward([[1,2,3]])\n",
    "print(softmax.output)\n",
    "softmax.forward([[ - 2 , - 1 , 0 ]]) # same idea as the values are just 0 negated\n",
    "print(softmax.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax.forward([[0.5, 1, 1.5]])\n",
    "print(softmax.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data( samples = 100 , classes = 3 )\n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense( 2 , 3 )\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense( 3 , 3 )\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Make a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Make a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Make a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "# Let's see output of the first few samples:\n",
    "print (activation2.output[: 5 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  [0 0 0 0 1 1 1 1]\n",
      "X:  [[ 0.          0.        ]\n",
      " [-0.12729016 -0.30807194]\n",
      " [ 0.51077586  0.4284302 ]\n",
      " [-0.9922136   0.12454799]\n",
      " [-0.         -0.        ]\n",
      " [ 0.09158143  0.32050577]\n",
      " [-0.66041565 -0.09108007]\n",
      " [ 0.8794786   0.47593853]]\n",
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-1.7768796e-05  4.4020140e-05  5.0318155e-05]\n",
      " [ 1.2585385e-05  9.4610687e-06  4.2363412e-05]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [ 1.1695344e-06  2.7461916e-05  4.8694048e-05]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [-3.5273497e-05  5.4776192e-05  4.4027292e-05]]\n",
      "[[1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [0.99993193 0.9999937  1.        ]\n",
      " [0.9999702  0.9999671  1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [0.9999525  0.9999788  1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [0.99990994 1.         0.9999893 ]]\n",
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33331892 0.33333948 0.3333416 ]\n",
      " [0.33333036 0.33332932 0.3333403 ]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33332512 0.33333388 0.33334097]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.3333145  0.33334455 0.33334097]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# run for a complete overview\n",
    "\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # init the weights and biases\n",
    "        \n",
    "        # set the weights\n",
    "        # https://www.mathsisfun.com/data/standard-normal-distribution.html, default mean = 0 and variance =1 \n",
    "        # this just creates a random from -3 to 3 , but multiplies it by 0.01 to gain a faster time to calculate\n",
    "        # NN prefer data between 1 and -1\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    " \n",
    "        # create an array of shape (1, n_neurons) filled with zeros\n",
    "        self.biases = np.zeros(( 1 , n_neurons))\n",
    "     \n",
    "    def forward(self, inputs):\n",
    "        # forward pass performs the dot added to the biases of course\n",
    "\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "\n",
    "# activations for the dense layer \n",
    "class Activation_ReLU:\n",
    "    \n",
    "    #forward pass\n",
    "    def forward(self, inputs):\n",
    "        #calculate output values from inputs, relu just takes max of 0 and the input whatever is larger\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "        \n",
    "# use exponential as its always positive and exagerrates the difference of inputs more as its a power        \n",
    "class Activation_Softmax:\n",
    "    #forward pass\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # get unnormalized probabilities\n",
    "        \n",
    "        # takes e^(inputs - largest value from inputs)\n",
    "        \n",
    "        # take the max value to become 0 , so its output is always 1 to indicate the highest \n",
    "        # prevent dead neurons and large exploding numbers\n",
    "        # if this was not done, then one value could be massive and make the sum\n",
    "\n",
    "        print(inputs)\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "        print(exp_values)\n",
    "       \n",
    "        # normalize them for each sample, takes all values and divides by the sum \n",
    "        probabilties = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "      \n",
    "        self.output = probabilties\n",
    " \n",
    "# returns 2 coordinates as features that represent each of classes\n",
    "# repeats this for how many times the samples go down.\n",
    "# attemps to consruct the original graph and colour based on when we get to where we assign labels to it if supervised.\n",
    "X, y = spiral_data(samples = 4, classes = 2)\n",
    "\n",
    "print(\"y: \", y)\n",
    "print(\"X: \", X)\n",
    "\n",
    "# 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2,3)\n",
    "\n",
    "# create relu activation to be used in dense layer\n",
    "\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense( 3 , 3 )\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Make a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Make a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Make a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# the largest one here is the class to select\n",
    "print (activation2.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
