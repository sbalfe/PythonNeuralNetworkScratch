{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.        ]\n",
      " [-0.12729016 -0.30807194]\n",
      " [ 0.51077586  0.4284302 ]\n",
      " [-0.9922136   0.12454799]\n",
      " [-0.         -0.        ]\n",
      " [ 0.09158143  0.32050577]\n",
      " [-0.66041565 -0.09108007]\n",
      " [ 0.8794786   0.47593853]]\n",
      "[0 0 0 0 1 1 1 1]\n",
      "loss: 1.0986145\n",
      "acc: 0.375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# run for a complete overview\n",
    "\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # init the weights and biases\n",
    "        \n",
    "        # set the weights\n",
    "        # https://www.mathsisfun.com/data/standard-normal-distribution.html, default mean = 0 and variance =1 \n",
    "        # this just creates a random from -3 to 3 , but multiplies it by 0.01 to gain a faster time to calculate\n",
    "        # NN prefer data between 1 and -1\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    " \n",
    "        # create an array of shape (1, n_neurons) filled with zeros\n",
    "        self.biases = np.zeros(( 1 , n_neurons))\n",
    "     \n",
    "    def forward(self, inputs):\n",
    "        # forward pass performs the dot added to the biases of course\n",
    "\n",
    "        # inputs here would be various rows of input features\n",
    "        # each row is multplied with the weights which are of shape (input, neuron (i.ie outputs of this))\n",
    "        # this gets eatch row in the batch and applies the corresponding weights accordingly which alterrs the\n",
    "        # dimensionality (inputs, neurons) regardless\n",
    "        # inputs = np.dot([[10,10],\n",
    "        #                    [5,5]], [[10,5,1],\n",
    "        #                            [5,2,1]]) = [[10(10) + 5(5), 10(5) + 5(2), ... ],...]\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "       \n",
    "        \n",
    "\n",
    "# activations for the dense layer \n",
    "class Activation_ReLU:\n",
    "    \n",
    "    #forward pass\n",
    "    def forward(self, inputs):\n",
    "        #calculate output values from inputs, relu just takes max of 0 and the input whatever is larger\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "        \n",
    "# use exponential as its always positive and exagerrates the difference of inputs more as its a power        \n",
    "class Activation_Softmax:\n",
    "    #forward pass\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # get unnormalized probabilities\n",
    "        \n",
    "        # takes e^(inputs - largest value from inputs)\n",
    "        \n",
    "        # take the max value to become 0 , so its output is always 1 to indicate the highest \n",
    "        # prevent dead neurons and large exploding numbers\n",
    "        # if this was not done, then one value could be massive and make the sum\n",
    "        \n",
    "        # subtracting largest value from list of input values \n",
    "        # changes the output values to be from -inf to 0 always which keeps the output of exponential being 0 to 1\n",
    "\n",
    "        # purpose of exp > monotonic (higher input , higher output), caps to 0 > 1 as said above, focuses more on change \n",
    "        # rather than magnitude between each.\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "      \n",
    "       \n",
    "        # normalize them for each sample, takes all values and divides by the sum \n",
    "        probabilties = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "      \n",
    "        self.output = probabilties\n",
    " \n",
    "\n",
    "# Common loss class\n",
    "class Loss :\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate ( self , output , y ):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "    \n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy ( Loss ):\n",
    "# Forward pass\n",
    "    def forward ( self , y_pred , y_true ):\n",
    "        \n",
    "        # Number of samples in a batch\n",
    "        samples = len (y_pred)\n",
    "        \n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        \n",
    "        #np.clip just removes value lower and higher than the parameters respectively.\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7 , 1 - 1e-7 )\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len (y_true.shape) == 1 :\n",
    "            correct_confidences = y_pred_clipped[range (samples),y_true]\n",
    "            \n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        # shape = 2 means a 2D array of course\n",
    "        # sum the values along with themselves as to select them all.\n",
    "        elif len (y_true.shape) == 2 :\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true,axis = 1)\n",
    "\n",
    "        # Losses\n",
    "        # selected the values the network has outputted\n",
    "        # just calculate the negative log loss\n",
    "        # this just returns greater loss the closer the value is to 0, and less loss for closer to 1 i.e. the truth value\n",
    "        negative_log_likelihoods = - np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "        \n",
    "# returns 2 coordinates as features that represent each of classes\n",
    "# repeats this for how many times the samples go down.\n",
    "# attemps to consruct the original graph and colour based on when we get to where we assign labels to it if supervised.\n",
    "X, y = spiral_data(samples = 4, classes = 2)\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "# 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2,3)\n",
    "\n",
    "# create relu activation to be used in dense layer\n",
    "\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense( 3 , 3 )\n",
    "\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Make a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Make a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Make a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# the largest one here is the class to select\n",
    "#print (activation2.output)\n",
    "\n",
    "# Perform a forward pass through loss function\n",
    "# it takes the output of second dense layer here and returns loss\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "# Print loss value\n",
    "print ( 'loss:' , loss) # at the momennt the weights are all random so confidences will be approximately 0.33 across the board.\n",
    "\n",
    "# accuracy is just sayng how much we got right\n",
    "# first calculates the index with the largest value \n",
    "predictions = np.argmax(activation2.output, axis = 1 )\n",
    "\n",
    "# if 2 dimensionsal as in , if its not a list, which in this case ,its not\n",
    "# flattens if required\n",
    "if len (y.shape) == 2 :\n",
    "\n",
    "    y = np.argmax(y, axis = 1 )\n",
    "\n",
    "    \n",
    "# compare each index, as predictiosn == y returns 1 or 0 for each column then just mean this for total accuracy.\n",
    "accuracy = np.mean(predictions == y)\n",
    "# Print accuracy\n",
    "print ( 'acc:' , accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Probabilities of 3 samples\n",
    "softmax_outputs = np.array([[ 0.7 , 0.2 , 0.1 ],\n",
    "                            [ 0.5 , 0.1 , 0.4 ],\n",
    "                            [ 0.02 , 0.9 , 0.08 ]])\n",
    "\n",
    "# Target (ground-truth) labels for 3 samples\n",
    "class_targets = np.array([ 0 , 1 , 1 ])\n",
    "\n",
    "# Calculate values along second axis (across), remember argmax returns the index with the highest value\n",
    "predictions = np.argmax(softmax_outputs, axis = 1 )\n",
    "\n",
    "# If targets are one-hot encoded - convert them\n",
    "if len (class_targets.shape) == 2 :\n",
    "\tclass_targets = np.argmax(class_targets, axis = 1 )  \n",
    "    \n",
    "# True evaluates to 1; False to 0\n",
    "accuracy = np.mean(predictions == class_targets)\n",
    "print ( 'acc:' , accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1  0.1  0.02]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "\n",
    "softmax_outputs = np.array([[ 0.7 , 0.1 , 0.2 ],\n",
    "                            [ 0.1 , 0.5 , 0.4 ],\n",
    "                            [ 0.02 , 0.9 , 0.08 ]])\n",
    "\n",
    "class_targets = np.array([[ 1 , 0 , 0 ],\n",
    "                        [ 0 , 1 , 0 ],\n",
    "                        [ 0 , 1 , 0 ]])\n",
    "class_targets = np.array([1,0,0])\n",
    "\n",
    "print(softmax_outputs[range(len(softmax_outputs)), class_targets])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
